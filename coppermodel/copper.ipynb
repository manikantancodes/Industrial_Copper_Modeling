# %% [markdown]
# # Industrial Copper Modeling

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt
import warnings
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
import json
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesRegressor, RandomForestClassifier
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score

# %%
df = pd.read_csv(r"C:\Users\manik\Desktop\Copper Analysis\Copper_Set.xlsx - Result 1.csv")
df.head()
df_raw = df.copy(True)

# %%
df.head(20)

# %%
df.T

# %%
df.shape

# %%
df.info()

# %%
# Finding count of null values in each 
df.isna().sum()

# %%
# Finding unique values in each column
for i in list(df.columns):
    print(f"{i}  \t:_____________{df[i].nunique()}")

# %%
df[df["id"].isna()==True]['id']

# %%
# Checking the data of the 26th row.
df.iloc[26]

# %%
# To get the statistics of the data.
df.describe()

# %%
df.describe().T

# %% [markdown]
# # **PRE-PROCESSING**

# %% [markdown]
# # Converting Date Time Format

# %%
# Converting item_date and delivery date data into the correct format.
columns_to_convert_datetime = ['item_date', 'delivery date']
for i in columns_to_convert_datetime:
    df[i] = pd.to_datetime(df[i], format='%Y%m%d', errors='coerce').dt.date
df.head()

# %%
# Viewing the datatypes of each column.
df.dtypes

# %%
# Converting data type from object to date 
df['item_date'] = pd.to_datetime(df['item_date'], format='%Y-%m-%d')
df['delivery date']=pd.to_datetime(df['delivery date'],format='%Y-%m-%d')
df.head(2)

# %%
# Viewing the datatypes of each column after converting for confirmation
df.dtypes

# %%
print(df['item_date'].max())
print(df['item_date'].min())

# %%
print(df["delivery date"].max())
print(df['delivery date'].min())

# %%
df.dtypes

# %% [markdown]
# # Converting Numeric Values

# %%
df.dtypes

# %%
columns_to_convert_numeric = ["quantity tons"]
for i in columns_to_convert_numeric:
  df[i] = pd.to_numeric(df[i], errors='coerce')
df.head(2)

# %% [markdown]
# # Drop ID Column

# %%
# Dropping the ID column, since it is not important as per requirement.
df.drop(columns=['id'], inplace=True)
df.head(2)

# %% [markdown]
# # Material_ref Column

# %%
# Replacing values which start with "0000*" with NaN for material_ref column
df['material_ref'] = df['material_ref'].apply(lambda x: np.nan if str(x).startswith('00000') else x)
df.head(3)

# %%
df.isna().sum()

# %%
df.describe().T

# %% [markdown]
# In the above description, we can see negative values for 'quantity tons' and 'selling_price' which are not supposed to be there. So we need to change them from negative to positive.

# %%
df.dtypes

# %%
df.head(10)

# %% [markdown]
# # Change the Negative Values in the Columns

# %%
df['quantity tons'] = df['quantity tons'].apply(lambda x: np.nan if x <= 0 else x)
df['selling_price'] = df['selling_price'].apply(lambda x: np.nan if x <= 0 else x)
df.describe().T

# %% [markdown]
# # Fill NA Values

# %% [markdown]
# ## With Median

# %%
# Numerical datatype using median
df['quantity tons'].fillna(df['quantity tons'].median(), inplace=True)
df['customer'].fillna(df['customer'].median(), inplace=True)
df['application'].fillna(df['application'].median(), inplace=True)
df['thickness'].fillna(df['thickness'].median(), inplace=True)
df['selling_price'].fillna(df['selling_price'].median(), inplace=True)
df.head()

# %% [markdown]
# ## With Mode

# %%
# Object datatype using mode
df['item_date'].fillna(df['item_date'].mode().iloc[0], inplace=True)
df['country'].fillna(df['country'].mode().iloc[0], inplace=True)
df['status'].fillna(df['status'].mode().iloc[0], inplace=True)
df['material_ref'].fillna(df['material_ref'].mode().iloc[0], inplace=True)
df['delivery date'].fillna(df['delivery date'].mode().iloc[0], inplace=True)

# %%
df.head()

# %%
df.isna().sum()

# %%
df.dtypes

# %%
df.to_csv(r"C:\Users\manik\Desktop\Copper Analysis\Copper_Set.xlsx - Result 1.csv", index=False)

# %%
df_raw.head()

# %%
df_processed = df.copy(True)
df_processed.head()

# %% [markdown]
# ## **EDA**

# %% [markdown]
# To Visualize Outliers and Skewness

# %%
df_processed.head(2)

# %%
df_processed.dtypes

# %% [markdown]
# # Using Boxplot and Distplot

# %%
# Create a figure with 6 subplots
fig, axes = plt.subplots(7, 2, figsize=(10, 20))

sns.boxplot(ax=axes[0, 0], data=df_processed['quantity tons'])      # Quantity tons
axes[0, 0].set_title('Quantity tons - Outliers')
sns.distplot(ax=axes[0, 1], a=df_processed['quantity tons'])
axes[0, 1].set_title('Quantity tons - Skewness')

sns.boxplot(ax=axes[1, 0], data=df_processed['customer'])          # Customer
axes[1, 0].set_title('Customer - Outliers')
sns.distplot(ax=axes[1, 1], a=df_processed['customer'])
axes[1, 1].set_title('Customer - Skewness')

sns.boxplot(ax=axes[2, 0], data=df_processed['country'])           # Country
axes[2, 0].set_title('Country - Outliers')
sns.distplot(ax=axes[2, 1], a=df_processed['country'])
axes[2, 1].set_title('Country - Skewness')

sns.boxplot(ax=axes[3, 0], data=df_processed['application'])       # Application
axes[3, 0].set_title('Application - Outliers')
sns.distplot(ax=axes[3, 1], a=df_processed['application'])
axes[3, 1].set_title('Application - Skewness')

sns.boxplot(ax=axes[4, 0], data=df_processed['thickness'])        # Thickness
axes[4, 0].set_title('Thickness - Outliers')
sns.distplot(ax=axes[4, 1], a=df_processed['thickness'])
axes[4, 1].set_title('Thickness - Skewness')

sns.boxplot(ax=axes[5, 0], data=df_processed['width'])            # Width
axes[5, 0].set_title('Width - Outliers')
sns.distplot(ax=axes[5, 1], a=df_processed['width'])
axes[5, 1].set_title('Width - Skewness')

sns.boxplot(ax=axes[6, 0], data=df_processed['selling_price'])   # Selling price
axes[6, 0].set_title('Selling price - Outliers')
sns.distplot(ax=axes[6, 1], a=df_processed['selling_price'])
axes[6, 1].set_title('Selling price - Skewness')

plt.tight_layout()
plt.show()

# %% [markdown]
# # Using Violin Plot

# %%
def plot(df_processed, column):
    plt.figure(figsize=(10, 5))
    sns.violinplot(data=df_processed, x=column)
    plt.title(f'Violin Plot for {column}')
    plt.show()

# %%
for i in ['quantity tons', 'customer', 'country',  'application', 'thickness', 'width', 'selling_price']:
    plot(df_processed, i)

# %%
out_skew_data = df_processed.copy(True)

# %%
skewed_columns = ['quantity tons', 'thickness', 'width', 'selling_price']
for column in skewed_columns:
    neg = (out_skew_data[column] <= 0)
    print(f'{column} : {neg.sum()} ')
    out_skew_data.loc[neg, column] = np.nan

# %%
out_skew_data.isna().sum()

# %%
out_skew_data.dropna(inplace=True)

# %%
out_skew_data.isna().sum()

# %%
out_skew_data_1 = out_skew_data.copy(True)

# %%
out_skew_data_1 = out_skew_data_1.drop(['item_date', 'delivery date', 'status', 'item type', 'material_ref'], axis=1)  # Removed material_ref column

# %%
out_skew_data

_1.head()

# %%
df.describe().T

# %%
# # Normality Test using Log transformation
for i in ['quantity tons', 'customer', 'country', 'application', 'thickness', 'width', 'selling_price']:
    out_skew_data_1[f'{i}_log'] = np.log1p(out_skew_data_1[i])
    plot(out_skew_data_1, f'{i}_log')

# %% [markdown]
# ## Outlier Test

# %%
from scipy import stats

# Function to detect outliers
def detect_outliers(data, column):
    z_scores = stats.zscore(data[column])
    abs_z_scores = np.abs(z_scores)
    outliers = (abs_z_scores > 3)
    return data[outliers]

# %% [markdown]
# Check if there are outliers in the 'quantity tons', 'customer', 'country', 'application', 'thickness', 'width', 'selling_price' columns

# %%
columns_to_check = ['quantity tons', 'customer', 'country', 'application', 'thickness', 'width', 'selling_price']

for column in columns_to_check:
    outliers = detect_outliers(out_skew_data_1, column)
    print(f"Number of outliers in {column}: {outliers.shape[0]}")

# %% [markdown]
# # Remove Outliers

# %%
# Function to remove outliers
def remove_outliers(data, column):
    z_scores = stats.zscore(data[column])
    abs_z_scores = np.abs(z_scores)
    non_outliers = (abs_z_scores <= 3)
    return data[non_outliers]

# %%
for column in columns_to_check:
    out_skew_data_1 = remove_outliers(out_skew_data_1, column)

# %% [markdown]
# # After Removing Outliers

# %%
for i in ['quantity tons', 'customer', 'country', 'application', 'thickness', 'width', 'selling_price']:
    plot(out_skew_data_1, f'{i}_log')

# %% [markdown]
# # **Correlation**

# %% [markdown]
# ## Correlation Analysis

# %%
out_skew_data_1.head()

# %%
# Correlation matrix
corr_matrix = out_skew_data_1.corr()

# %%
corr_matrix['selling_price_log'].sort_values(ascending=False)

# %% [markdown]
# ## Heatmap

# %%
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

# %% [markdown]
# ## Pairplot

# %%
sns.pairplot(out_skew_data_1)
plt.show()

# %% [markdown]
# # **Machine Learning Models**

# %% [markdown]
# ## **Train Test Split**

# %%
from sklearn.model_selection import train_test_split

# %%
out_skew_data_1.head()

# %%
# Splitting data into training and testing sets
X = out_skew_data_1.drop(['selling_price_log'], axis=1)
y = out_skew_data_1['selling_price_log']

# %%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# %% [markdown]
# # Model Selection

# %% [markdown]
# ## Decision Tree Regression

# %%
# Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor
dtr = DecisionTreeRegressor(random_state=42)
dtr.fit(X_train, y_train)

# Predictions
y_pred_dtr = dtr.predict(X_test)

# Evaluation
mse_dtr = mean_squared_error(y_test, y_pred_dtr)
mae_dtr = mean_absolute_error(y_test, y_pred_dtr)
r2_dtr = r2_score(y_test, y_pred_dtr)

print(f"Decision Tree Regressor - MSE: {mse_dtr}, MAE: {mae_dtr}, R2: {r2_dtr}")

# %% [markdown]
# ## Random Forest Regression

# %%
# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor(random_state=42)
rfr.fit(X_train, y_train)

# Predictions
y_pred_rfr = rfr.predict(X_test)

# Evaluation
mse_rfr = mean_squared_error(y_test, y_pred_rfr)
mae_rfr = mean_absolute_error(y_test, y_pred_rfr)
r2_rfr = r2_score(y_test, y_pred_rfr)

print(f"Random Forest Regressor - MSE: {mse_rfr}, MAE: {mae_rfr}, R2: {r2_rfr}")

# %% [markdown]
# ## XGBoost Regression

# %%
# XGBoost Regressor
from xgboost import XGBRegressor
xgbr = XGBRegressor(random_state=42)
xgbr.fit(X_train, y_train)

# Predictions
y_pred_xgbr = xgbr.predict(X_test)

# Evaluation
mse_xgbr = mean_squared_error(y_test, y_pred_xgbr)
mae_xgbr = mean_absolute_error(y_test, y_pred_xgbr)
r2_xgbr = r2_score(y_test, y_pred_xgbr)

print(f"XGBoost Regressor - MSE: {mse_xgbr}, MAE: {mae_xgbr}, R2: {r2_xgbr}")

# %% [markdown]
# ## Model Comparison

# %%
models = pd.DataFrame({
    'Model': ['Decision Tree Regressor', 'Random Forest Regressor', 'XGBoost Regressor'],
    'MSE': [mse_dtr, mse_rfr, mse_xgbr],
    'MAE': [mae_dtr, mae_rfr, mae_xgbr],
    'R2': [r2_dtr, r2_rfr, r2_xgbr]
})

models.sort_values(by='R2', ascending=False)

# %% [markdown]
# # Best Model: XGBoost Regressor

# %% [markdown]
# The best performing model is XGBoost Regressor with the highest R2 score. 

# %% [markdown]
# # Save Best Model

# %%
import joblib

# Save the model as a pickle file
joblib.dump(xgbr, 'xgboost_model.pkl')

# %% [markdown]
# # **Conclusion**

# %% [markdown]
# In this analysis, we performed data cleaning, preprocessing, exploratory data analysis, and built machine learning models to predict the selling price of industrial copper products. After comparing various models, we found that the XGBoost Regressor performed the best with the highest R2 score, indicating it as the best model for our dataset.

# %% [markdown]
# The steps followed in this analysis include:

# %% [markdown]
# 1. Data Loading and Exploration
# 2. Data Cleaning and Preprocessing
# 3. Exploratory Data Analysis (EDA)
# 4. Feature Engineering and Transformation
# 5. Model Building and Evaluation
# 6. Model Comparison
# 7. Saving the Best Model